{
  "doc-24516f15972a0a0c6bc56a24e20baa4a": {
    "content": "Service Design and Decomposition\nEach service must be designed around a single business capability. If the service needs to expose \nfunctionality externally, use REST APIs. If it communicates primarily with other internal services, \nuse gRPC for better performance. Services should remain stateless, with session data stored \nexternally. If services must coordinate to complete a workflow, evaluate the transaction model. If \nstrong consistency is required, apply the Saga pattern. Otherwise, use asynchronous events and \ncompensating transactions. Services that require eventual consistency must include retry logic and \nfailure handling. If the design involves stateful behavior, skip directly to Data Persistence and \nManagement to ensure persistence requirements are defined early. Otherwise, continue to \nContainerization.\nContainerization\nAll services must be containerized using Docker. Use multi-stage builds to separate build-time and \nruntime dependencies. If container images will be deployed to secure environments, apply \nvulnerability scanning during the CI process. Never include secrets or configuration values in the \ncontainer image. If your container needs access to runtime secrets, ensure you're integrating with a \nsecrets manager. If you are containerizing services that have runtime-sidecar dependencies (e.g., for\nservice mesh or observability), include them now, or skip directly to Service Discovery and Routing\nto configure routing accordingly. If containers are built with runtime-based configuration, go \ndirectly to Service Discovery and Routing. Otherwise, continue to Orchestration with Kubernetes.\nOrchestration with Kubernetes\nEach service must be deployed as a Kubernetes Deployment. Use ConfigMaps for configuration \nand Secrets for sensitive values. If the service requires persistent volumes or must retain state, use \nStatefulSets instead of Deployments. In this case, before continuing, go to Data Persistence and \nManagement to prepare persistent data handling. All services must define liveness and readiness \nprobes. If the service is stateless, apply autoscaling policies using the Horizontal Pod Autoscaler. If \nit is stateful, scaling must be manually configured and tested for consistency. After orchestration is \ncomplete, proceed to Service Discovery and Routing.\nService Discovery and Routing\nServices must register and resolve via Kubernetes DNS. If your system requires advanced traffic \nrouting, retries, or circuit breakers, implement a service mesh like Istio. If Istio is installed, define \nVirtualServices for routing and DestinationRules for load balancing. If a service mesh is not used, \nconfigure ingress and internal routing via kube-proxy or an Ingress controller. If the service will be \nexposed externally, continue to API Gateway and External Access. If the service is internal-only, \nskip to Observability to begin configuring logging and monitoring.\nAPI Gateway and External Access\nPublic-facing services must be routed through an API gateway such as Kong or Ambassador. \nConfigure routes with JWT-based authentication and enable rate limiting. If the environment \nrequires integration with multiple identity providers, federate access using OAuth2 and OpenID \nConnect. If request routing is tenant-specific, define routing rules by header or path. If this service \nrequires secure user data access, proceed to Security before exposing the service. Once gateway \nrouting is configured, continue to Observability.\nObservability\nAll services must emit structured logs and metrics. Deploy Fluent Bit or Fluentd to capture logs and\nforward them to Elasticsearch or Loki. Prometheus must collect metrics from services exposing \na /metrics endpoint. Set up Alertmanager to monitor error rate, latency, and throughput. If tracing is \nrequired, inject OpenTelemetry into services and export data to Jaeger. Dashboards must be created \nin Grafana. If services are mission-critical, observability setup must be completed before \nimplementing CI/CD and Deployment Strategies, as monitoring will control deployment rollbacks. \nOnce observability is in place, continue to Security.\nSecurity\nAll internal traffic must be encrypted using mutual TLS. If a service mesh is used, enable mesh\u0002wide mTLS policies. Otherwise, enforce TLS termination at ingress and configure TLS downstream\nusing application-level encryption or sidecar proxies. Role-Based Access Control must be applied in\nKubernetes, and each namespace should define strict NetworkPolicies to control service-to-service \ncommunication. All container images must pass vulnerability scanning, and sensitive data must be \nencrypted in transit and at rest. If any service manages personal data or secrets, validate whether it \nrequires data protection rules, and if so, return to Data Persistence and Management to configure \nsecure persistence. After security enforcement is in place, continue to CI/CD and Deployment \nStrategies.\nCI/CD and Deployment Strategies\nThe CI pipeline must validate code, run tests, and scan for vulnerabilities. The CD pipeline must \nsupport rolling updates by default. For customer-facing services, implement canary deployments or \nblue/green rollouts. Canary releases must integrate with observability to control rollback triggers. If \nfeature flags are used, integrate a central feature flag system. If one is not available, return to \nService Design and Decomposition to review how toggles are implemented in code. All rollback \nlogic must be automated. Once deployment automation is verified, continue to Resilience and \nFailover.\nResilience and Failover\nServices must be able to detect failure and isolate it. Implement circuit breakers to stop cascading \nfailures. For messaging services, configure dead-letter queues and define idempotent retry logic. If \ndownstream services are unavailable, the system must fall back to safe degraded modes, such as \nserving cached data or skipping non-essential operations. If traffic rerouting is required, return to \nService Discovery and Routing and update routing logic in the service mesh or ingress. Once \nresilience policies are in place, continue to Data Persistence and Management.\nData Persistence and Management\nAll persistent data must be stored in secure, reliable databases. Use PostgreSQL or MySQL for \ntransactional workloads, and Cassandra or DynamoDB for distributed write-heavy systems. Redis \nshould be used for ephemeral or cached data. Databases must support automated backups and point\u0002in-time recovery. If deploying across regions, databases must support geo-replication. Ensure \nencryption at rest is enabled. If any service handles personal or regulated data, return to Security to \nvalidate encryption and retention policies. After persistence strategy is complete, continue to \nDisaster Recovery and Multi-Region Deployment.\nDisaster Recovery and Multi-Region Deployment\nIn multi-region deployments, use cloud-native load balancers with health checks and failover. Each \nregion must operate autonomously with local instances of critical services like authentication, \nconfiguration, and observability. Route global traffic based on latency or availability. Ensure that \npersistent storage is replicated across regions or backed up regularly. Validate recovery plans \nthrough quarterly simulations. If failover recovery depends on Kubernetes orchestration, return to \nOrchestration with Kubernetes to ensure the control plane is configured for high availability."
  }
}